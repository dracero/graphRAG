{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las dependencias que tuve que ir instalando las fuí comentado para que quede registrado\n",
    "Funcione con python 3.11 porque knowledge_graph_maker solo funciona con esa versión de python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install knowledge-graph-maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j wikipedia tiktoken yfiles_jupyter_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuve que abrir una cuenta en Groq para obtener una API KEY\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_graph_maker import GraphMaker, Ontology, GroqClient\n",
    "from knowledge_graph_maker import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta parte es donde se pone a mano el pdf que se quiere analizar, esto hay que hacerlo con fasAPI \n",
    "#para que reciba con el verbo POST el pdf por el body.\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from typing import List\n",
    "\n",
    "def scrap_pdf(pdf_path: str) -> List[str]:\n",
    "    # Paso 1: Cargar el PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Paso 2: Extraer el texto de cada página y almacenarlo en una lista\n",
    "    text_list = [page.page_content for page in pages]\n",
    "    \n",
    "    return text_list\n",
    "\n",
    "# Uso de la función\n",
    "#pdf_path = \"http://www.fiumsa.edu.bo/docentes/mramirez/capitulo_II.pdf\"\n",
    "pdf_path = \"https://www.williammurdoch.bham.sch.uk/admin/ckfinder/userfiles/files/Y5%20Reading%20Task%20-%20Elizabeth%20I.pdf\"\n",
    "#pdf_path = \"https://cms.fi.uba.ar/uploads/Informacion_sobre_Doble_Diploma_docx_4eb4306159.pdf\"\n",
    "scraped_text = scrap_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ontology = Ontology(\\n    labels=[\\n        {\"Person\": \"Nombres de personas, incluyendo estudiantes, profesores y autoridades.\"},\\n        {\"Program\": \"Nombres de programas educativos, como \\'Doble Diploma\\' o nombres específicos de carreras o disciplinas.\"},\\n        {\"Institution\": \"Nombres de instituciones educativas involucradas, como la \\'Facultad de Ingeniería\\', \\'Universidad de Buenos Aires\\', o universidades extranjeras.\"},\\n        {\"Country\": \"Nombres de países involucrados en los programas, como \\'Argentina\\' o cualquier país extranjero mencionado.\"},\\n        {\"Document\": \"Documentos formales o referencias a normativas, acuerdos, o reglamentos.\"},\\n        {\"Date\": \"Fechas importantes mencionadas en el contexto del programa, como fechas límite o de inicio de actividades.\"},\\n        {\"Requirement\": \"Criterios o condiciones que los estudiantes deben cumplir para participar en el programa.\"},\\n        {\"Procedure\": \"Pasos o procesos que los estudiantes deben seguir para aplicar o completar el programa de Doble Diploma.\"},\\n        {\"Miscellaneous\": \"Cualquier otro concepto relevante que no se categorice bajo las etiquetas anteriores, como términos específicos del programa o conceptos relacionados a intercambios académicos.\"},\\n    ],\\n    relationships=[\\n        \"Association between a Person and a Program\",\\n        \"Connection between an Institution and a Program\",\\n        \"Link between a Country and an Institution\",\\n        \"Requirement or condition related to a Program\",\\n        \"Procedure or step involved in the application or completion of the Program\",\\n    ],\\n)'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#La ontología, se tiene que poner como archivo, en principo ponemos el texto en chatGPT y pedimos que nos genere una ontología\n",
    "#debe pasarse el json en forma por body en un POST a la API de fasAPI.\n",
    "ontology = Ontology(\n",
    "    labels=[\n",
    "        {\"Person\": \"Person name without any adjectives, Remember a person may be referenced by their name or using a pronoun\"},\n",
    "        {\"Object\": \"Do not add the definite article 'the' in the object name\"},\n",
    "        {\"Event\": \"Event event involving multiple people. Do not include qualifiers or verbs like gives, leaves, works etc.\"},\n",
    "        \"Place\",\n",
    "        \"Document\",\n",
    "        \"Organisation\",\n",
    "        \"Action\",\n",
    "        {\"Miscellaneous\": \"Any important concept can not be categorised with any other given label\"},\n",
    "    ],\n",
    "    relationships=[\n",
    "        \"Relation between any pair of Entities\"\n",
    "        ],\n",
    ")\n",
    "\n",
    "'''ontology = Ontology(\n",
    "    labels=[\n",
    "        {\"Person\": \"Nombres de personas, incluyendo estudiantes, profesores y autoridades.\"},\n",
    "        {\"Program\": \"Nombres de programas educativos, como 'Doble Diploma' o nombres específicos de carreras o disciplinas.\"},\n",
    "        {\"Institution\": \"Nombres de instituciones educativas involucradas, como la 'Facultad de Ingeniería', 'Universidad de Buenos Aires', o universidades extranjeras.\"},\n",
    "        {\"Country\": \"Nombres de países involucrados en los programas, como 'Argentina' o cualquier país extranjero mencionado.\"},\n",
    "        {\"Document\": \"Documentos formales o referencias a normativas, acuerdos, o reglamentos.\"},\n",
    "        {\"Date\": \"Fechas importantes mencionadas en el contexto del programa, como fechas límite o de inicio de actividades.\"},\n",
    "        {\"Requirement\": \"Criterios o condiciones que los estudiantes deben cumplir para participar en el programa.\"},\n",
    "        {\"Procedure\": \"Pasos o procesos que los estudiantes deben seguir para aplicar o completar el programa de Doble Diploma.\"},\n",
    "        {\"Miscellaneous\": \"Cualquier otro concepto relevante que no se categorice bajo las etiquetas anteriores, como términos específicos del programa o conceptos relacionados a intercambios académicos.\"},\n",
    "    ],\n",
    "    relationships=[\n",
    "        \"Association between a Person and a Program\",\n",
    "        \"Connection between an Institution and a Program\",\n",
    "        \"Link between a Country and an Institution\",\n",
    "        \"Requirement or condition related to a Program\",\n",
    "        \"Procedure or step involved in the application or completion of the Program\",\n",
    "    ],\n",
    ")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Groq models. Se puede probar con el que mejor funcione, hay que probar.\n",
    "model = \"mixtral-8x7b-32768\"\n",
    "#model=\"gemma-7b-it\"\n",
    "\n",
    "## Open AI models\n",
    "##oai_model=\"gpt-3.5-turbo\"\n",
    "\n",
    "## Use Groq\n",
    "llm = GroqClient(model=model, temperature=0.1, top_p=0.5)\n",
    "## OR Use OpenAI\n",
    "##llm = OpenAIClient(model=oai_model, temperature=0.1, top_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto es interno, acá se genera el grafo\n",
    "import datetime\n",
    "current_time = str(datetime.datetime.now())\n",
    "graph_maker = GraphMaker(ontology=ontology, llm_client=llm, verbose=False)\n",
    "\n",
    "def generate_summary(text):\n",
    "    SYS_PROMPT = (\n",
    "        \"Succintly summarise the text provided by the user. \"\n",
    "        \"Respond only with the summary and no other comments\"\n",
    "    )\n",
    "    try:\n",
    "        summary = llm.generate(user_message=text, system_message=SYS_PROMPT)\n",
    "    except:\n",
    "        summary = \"\"\n",
    "    finally:\n",
    "        return summary\n",
    "\n",
    "docs = map(\n",
    "    lambda t: Document(text=t, metadata={\"summary\": generate_summary(t), 'generated_at': current_time}),\n",
    "    scraped_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_maker.from_documents(\n",
    "    list(docs), \n",
    "    delay_s_between=0 ## delay_s_between because otherwise groq api maxes out pretty fast. \n",
    "    ) \n",
    "print(\"Total number of Edges\", len(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este no hace falta en el archivo, es para ver el grafo en consola.\n",
    "for edge in graph:\n",
    "    print(edge.model_dump(exclude=['metadata']), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credenciales de Neo4j\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv(\"NEO4J_USERNAME\")\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv(\"NEO4J_PASSWORD\")\n",
    "os.environ[\"NEO4J_URI\"]  = os.getenv(\"NEO4J_URI\")\n",
    "os.environ[\"AURA_INSTANCEID\"]=\"31995b6a\"\n",
    "os.environ[\"AURA_INSTANCENAME\"]=\"Instance01\"'The Doble Diploma (DD) is a joint engineering degree program between France and a specific institution, such as FIUBA. To be eligible, students must meet requirements such as having a good academic performance, a minimum level of English and French, and approving at least 60% of their career (excluding the CBC). The application process typically takes place in June-July of the year N-1.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se guarda el grafo en Neo4j\n",
    "from knowledge_graph_maker import Neo4jGraphModel\n",
    "\n",
    "create_indices = False\n",
    "neo4j_graph = Neo4jGraphModel(edges=graph, create_indices=create_indices)\n",
    "\n",
    "neo4j_graph.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install yfiles_jupyter_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este no es es necesario, es para ver el grafo en consola. En la API de backend esto no va.\n",
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "import os\n",
    "\n",
    "# directly show the graph resulting from the given Cypher query\n",
    "default_cypher = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    #display(widget)\n",
    "    return widget\n",
    "\n",
    "showGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain_huggingface --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/msmarco-distilbert-base-tas-b\n"
     ]
    }
   ],
   "source": [
    "#Se arman los embeddings de Hugginface\n",
    "\n",
    "#Esto se tiene que sejecutar solo una vez, si los ebeddings ya estàn creados genera un error\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Inicializa el modelo y el tokenizador de Hugging Face\n",
    "model_name = \"sentence-transformers/msmarco-distilbert-base-tas-b\"\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Configura el token de relleno\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Asigna eos_token como pad_token\n",
    "\n",
    "# Configura las credenciales de Neo4j\n",
    "url = os.environ.get(\"NEO4J_URI\")\n",
    "username = os.environ.get(\"NEO4J_USERNAME\")\n",
    "password = os.environ.get(\"NEO4J_PASSWORD\")\n",
    "\n",
    "# Verifica si las credenciales están configuradas\n",
    "if not url or not username or not password:\n",
    "    raise ValueError(\"Las credenciales de Neo4j no están configuradas correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se guardan los embeddings en Neo4j para hacer una búsqueda híbrida.\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    embeddings_model,\n",
    "    search_type=\"hybrid\",\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    node_label=\"Entity\",\n",
    "    text_node_properties=[\"name\",\"label\"],  # Asegúrate de que esto esté correcto\n",
    "    embedding_node_property=\"embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay que analizar si esto es necesario en la api de backend\n",
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Configuración de Neo4j\n",
    "graph = Neo4jGraph()\n",
    "graph.query(\n",
    "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
    "\n",
    "# Extract entities from text\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, organization, or business entities that \"\n",
    "        \"appear in the text\",\n",
    "    )\n",
    "\n",
    "# Configuración del modelo Groq\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"mixtral-8x7b-32768\",  # Asegúrate de usarl\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting organization and person entities from the text.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay que ver si esto es necesario en la API de backend\n",
    "entity_chain.invoke({\"question\": \"Qué requsitos debo cumplir para tener la doble diplomatura?\"}).names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]# Configura la conexión a la base de datos Neo4j\n",
    "\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "        full_text_query += f\" {words[-1]}~2\"\n",
    "    return full_text_query.strip()\n",
    "\n",
    "# Fulltext index query\n",
    "def structured_retriever(question: str) -> str:\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"\n",
    "            CALL db.index.fulltext.queryNodes(\"keyword\", $query) YIELD node, score\n",
    "            WITH node\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r]->(neighbor)\n",
    "              WHERE NOT type(r) = 'MENTIONS'\n",
    "              RETURN node.name + ' - ' + type(r) + ' -> ' + neighbor.name AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r]-(neighbor)\n",
    "              WHERE NOT type(r) = 'MENTIONS'\n",
    "              RETURN neighbor.name + ' - ' + type(r) + ' -> ' +  node.name AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": entity},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result\n",
    "\n",
    "# Prueba la función, se puede probar con cualquier pregunta. Esto no va en la API de backend, pero lo de arriba si.\n",
    "print(structured_retriever(\"Que es la doble diplomatura?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No va en la API\n",
    "print(structured_retriever(\"Qué es la doble diplomatura?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Va en la API es interno.\n",
    "def retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = []\n",
    "    for el in vector_index.similarity_search(question):\n",
    "        if el.page_content:\n",
    "            unstructured_data.append(el.page_content)\n",
    "        else:\n",
    "            print(f\"Warning: Missing or empty text in node {el.metadata['node_id']}\")\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \".join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Va en la API\n",
    "vector_index = Neo4jVector.from_existing_index(\n",
    "    index_name=\"keyword\",\n",
    "    embedding=embeddings_model,  # Añadimos esto\n",
    "    node_label=\"Entity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historio del chatbot, va en la API.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.runnables import  RunnableParallel, RunnablePassthrough\n",
    "from typing import Tuple, List\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatGroq(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Va en la API\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto es necesario para borrar los nodos con ebeddings vacìos. Va en la API porque sino da una falla la query.\n",
    "# Hay que poner esto para que no existan nulos y se puea hacer la consulta.\n",
    "# Inicia la conexión con la base de datos\n",
    "driver = GraphDatabase.driver(url, auth=(username, password))\n",
    "\n",
    "# Define la función para ejecutar la consulta\n",
    "def update_entities_text(tx):\n",
    "    query = \"\"\"\n",
    "    MATCH (n:Entity)\n",
    "    WHERE n.text IS NULL OR n.text = \"\"\n",
    "    SET n.text = \"Texto predeterminado\"\n",
    "    RETURN n\n",
    "    \"\"\"\n",
    "    result = tx.run(query)\n",
    "    return [record[\"n\"] for record in result]\n",
    "\n",
    "# Ejecuta la consulta dentro de una sesión\n",
    "with driver.session() as session:\n",
    "    entities = session.write_transaction(update_entities_text)\n",
    "\n",
    "# Muestra los resultados\n",
    "for entity in entities:\n",
    "    print(entity)\n",
    "\n",
    "# Cierra la conexión\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta pregunta tiene que entrar por la API de backend usando el verbo POST de fastAPI.\n",
    "chain.invoke({\"question\": \"Recién aprobé cuatro materias, me puede presentar a la doble diplomatura?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
